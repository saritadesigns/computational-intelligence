{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This code generates data that follows a function, then attempts to predict the function using KFold validation to determine the optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from tensorflow.keras.metrics import Recall\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data\n",
    "Two functions can be used, defined with the \"function\" switch input.\n",
    "The data is then split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data\n",
    "def generate_data(size,function):\n",
    "    #Create data given size:\n",
    "    dimensions = (size,1)    \n",
    "    if function == 1:\n",
    "        x = np.random.uniform(low=-1,high=1,size=dimensions)\n",
    "        f = lambda x: x*math.sin(6*math.pi*x)*math.exp(-(x**2))\n",
    "    else:\n",
    "        x = np.random.uniform(low=-2,high=2,size=dimensions)\n",
    "        f = lambda x: math.exp(-(x**2))*math.atan(x)*math.sin(4*math.pi*x)\n",
    "        \n",
    "    fun = np.vectorize(f)\n",
    "    y = fun(x)\n",
    "    \n",
    "    #Split into Train/Validate (80%) and Test (20%)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    return (x_train,y_train), (x_test,y_test)\n",
    "\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = generate_data(10000,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "The NN is defiend with KFold and early stopping for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Neural Network:\n",
    "#one hidden layer with variable nodes; sigmoid activation function\n",
    "def compile_model(no_hidden_nodes):\n",
    "    model = keras.models.Sequential()\n",
    "    #hidden layer\n",
    "    ki=keras.initializers.RandomNormal(mean=0., stddev=30)\n",
    "    bi=keras.initializers.RandomNormal(mean=0., stddev=10)\n",
    "    model.add(Dense(\n",
    "        no_hidden_nodes,\n",
    "        activation='sigmoid',\n",
    "        input_shape=(1,),\n",
    "        kernel_initializer=ki,\n",
    "        bias_initializer=bi)) \n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(1, activation='linear')) #output layer\n",
    "\n",
    "    optimizer = Adam(lr=0.01)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "model_best_train = np.zeros((4,4))\n",
    "model_best_validate = np.zeros((4,4))\n",
    "model_avg_train = np.zeros((4,4))\n",
    "model_avg_validate = np.zeros((4,4))\n",
    "\n",
    "kfold = KFold(n_splits=10,shuffle=True)\n",
    "for i,size in enumerate(size_data):\n",
    "    inputs = x_train[:size]\n",
    "    targets = y_train[:size]\n",
    "    \n",
    "    for j,node_num in enumerate(hidden_nodes):\n",
    "        print(f'Model {i},{j}')\n",
    "        fold_no = 1\n",
    "        model_training_error = []\n",
    "        model_validation_error = []\n",
    "\n",
    "#         for i in range(5):\n",
    "        #     Shuffle data\n",
    "        for train_idx, validate_idx in kfold.split(inputs):\n",
    "            inputs_train, inputs_val = inputs[train_idx], inputs[validate_idx]\n",
    "            targets_train, targets_val = targets[train_idx], targets[validate_idx]\n",
    "\n",
    "            compile_model(node_num)\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "            #Fit to model\n",
    "            es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=200)\n",
    "            h = model.fit(\n",
    "                inputs_train, targets_train,\n",
    "                validation_data=(inputs_val,targets_val),\n",
    "                epochs=10,\n",
    "                batch_size=10,\n",
    "                verbose=0,\n",
    "                callbacks=[es])\n",
    "            \n",
    "            #Metrics\n",
    "            training_score = model.evaluate(inputs_train, targets_train, verbose=0)\n",
    "            validation_score = model.evaluate(inputs_val, targets_val, verbose=0)\n",
    "\n",
    "            model_training_error.append(training_score * 100)\n",
    "            model_validation_error.append(validation_score * 100)\n",
    "\n",
    "            #Increase fold number\n",
    "            fold_no = fold_no + 1\n",
    "                        \n",
    "\n",
    "        model_best_train[i][j] = min(model_training_error)\n",
    "        model_best_validate[i][j] = min(model_validation_error)\n",
    "        model_avg_train[i][j] = sum(model_training_error)/len(model_training_error)\n",
    "        model_avg_validate[i][j] = sum(model_validation_error)/len(model_validation_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Model\n",
    "In this case, the model M32 is selected as the most generalzied model (least overfit) given the validation error. The full training set is applied to the model. Then the testing data is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate validation/training graph for Model M32\n",
    "\n",
    "inputs = x_train[:200] #M32\n",
    "for train_idx, validate_idx in kfold.split(inputs):\n",
    "    inputs_train, inputs_val = inputs[train_idx], inputs[validate_idx]\n",
    "    targets_train, targets_val = targets[train_idx], targets[validate_idx]\n",
    "    \n",
    "    compile_model(40) #M32\n",
    "\n",
    "    #Fit to model\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=200)\n",
    "    h = model.fit(\n",
    "        inputs_train, targets_train,\n",
    "        validation_data=(inputs_val,targets_val),\n",
    "        epochs=100,\n",
    "        batch_size=10,\n",
    "        verbose=0,\n",
    "        callbacks=[es])\n",
    "\n",
    "plt.plot(np.log10(h.history['loss']))\n",
    "plt.plot(np.log10(h.history['val_loss']), 'r')\n",
    "plt.legend(['train loss', 'val loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Full dataset to Model 02\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "#hidden layer\n",
    "ki=keras.initializers.RandomNormal(mean=0., stddev=30)\n",
    "bi=keras.initializers.RandomNormal(mean=0., stddev=10)\n",
    "model.add(Dense(\n",
    "    40,\n",
    "    activation='sigmoid',\n",
    "    input_shape=(1,),\n",
    "    kernel_initializer=ki,\n",
    "    bias_initializer=bi)) \n",
    "\n",
    "#output layer\n",
    "model.add(Dense(1, activation='linear')) #output layer\n",
    "\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "h = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=10,\n",
    "    verbose=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check test data\n",
    "y_pred = model(x_test)\n",
    "plt.scatter(x_test,y_test,label=\"Original function\")\n",
    "\n",
    "plt.scatter(x_test,y_pred,label='Model prediction with test data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
