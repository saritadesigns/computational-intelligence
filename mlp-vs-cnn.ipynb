{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This code analyzes the differences in performance for various CNNs (Convolutional Neural Networks) and an MLP (Multi Layer Perceptron) for the CIFAR10 dataset. The purpose is to analyze the effect of layers and hidden nodes on the efficiency and accuracy of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "#(MacOS user)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "\n",
    "def get_random_data(x,y,sample_percent):\n",
    "    total = y_train.shape[0]\n",
    "    sample_amount = int(total*(sample_percent/100))\n",
    "    \n",
    "    # returns an array of `sample_amount` length with random values from [0,`total`] \n",
    "    random_indices = np.random.choice(total,sample_amount)\n",
    "    x= x[random_indices]\n",
    "    y= y[random_indices]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "#Normalization\n",
    "def normalize(x):\n",
    "    return (x - x.min(axis=0)) / (x.max(axis=0) - x.min(axis=0))\n",
    "\n",
    "\n",
    "x_train, y_train = get_random_data(x_train,y_train,20)\n",
    "\n",
    "#one hot encode\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "#normalize\n",
    "x_train = normalize(x_train.astype(np.float32))\n",
    "x_test = normalize(x_test.astype(np.float32))\n",
    "\n",
    "print('train images shape:', x_train.shape)\n",
    "print('train labels shape:', y_train.shape)\n",
    "print('test images shape:', x_test.shape)\n",
    "print('test labels shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first few images\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.title(y_test[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build 3 Models\n",
    "(1) MLP\n",
    "<br> (2) CNN-1\n",
    "<br> (3) CNN-2\n",
    "<br> and compare the three on performance and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build 3 models, batchsize=32, Adam, categorical loss, accuracy, 5 epochs\n",
    "\n",
    "def compile_model(model):\n",
    "    return model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "def fit_model(model,x_train,y_train,x_test,y_test):\n",
    "    return model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(x_test,y_test),\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Build an MLP: Dense(512,sigmoid), Dense(10,activation)\n",
    "mlp = Sequential()\n",
    "mlp.add(Flatten(input_shape=(32,32,3)))\n",
    "mlp.add(Dense(512,activation='sigmoid',input_shape=(3072,)))\n",
    "mlp.add(Dense(10,activation='softmax'))\n",
    "print(mlp.summary())\n",
    "\n",
    "compile_model(mlp)\n",
    "h = fit_model(mlp,x_train,y_train,x_test,y_test)\n",
    "plt.plot(h.history['accuracy'])\n",
    "\n",
    "plt.plot(h.history['accuracy'])\n",
    "plt.plot(h.history['val_accuracy'], 'r')\n",
    "plt.legend(['train acc', 'val acc'])\n",
    "print('train acc: %.2f %% \\nval acc: %.2f %%'%(h.history['accuracy'][-1]*100,h.history['val_accuracy'][-1]*100))\n",
    "\n",
    "plt.plot(np.log10(h.history['loss']))\n",
    "plt.plot(np.log10(h.history['val_loss']), 'r')\n",
    "plt.legend(['train loss', 'val loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) CNN\n",
    "cnn_2 = Sequential()\n",
    "cnn_2.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu',input_shape=(32,32,3)))\n",
    "cnn_2.add(Flatten())\n",
    "cnn_2.add(Dense(512,activation='sigmoid'))\n",
    "cnn_2.add(Dense(10,activation='softmax'))\n",
    "print(cnn_2.summary())\n",
    "\n",
    "compile_model(cnn_2)\n",
    "h = fit_model(cnn_2,x_train,y_train,x_test,y_test)\n",
    "plt.plot(h.history['accuracy'])\n",
    "print('Test accuracy: %.2f %%'%(100*cnn_2.evaluate(x_test,y_test,verbose=0)[1]))\n",
    "\n",
    "plt.plot(h.history['accuracy'])\n",
    "plt.plot(h.history['val_accuracy'], 'r')\n",
    "plt.legend(['train acc', 'val acc'])\n",
    "print('train acc: %.2f %% \\nval acc: %.2f %%'%(h.history['accuracy'][-1]*100,h.history['val_accuracy'][-1]*100))\n",
    "\n",
    "plt.plot(np.log10(h.history['loss']))\n",
    "plt.plot(np.log10(h.history['val_loss']), 'r')\n",
    "plt.legend(['train loss', 'val loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) CNN\n",
    "cnn_3 = Sequential()\n",
    "cnn_3.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu',input_shape=(32,32,3)))\n",
    "cnn_3.add(MaxPool2D())\n",
    "cnn_3.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\n",
    "cnn_3.add(MaxPool2D())\n",
    "cnn_3.add(Flatten())\n",
    "cnn_3.add(Dense(512,activation='sigmoid'))\n",
    "cnn_3.add(Dropout(0.2))\n",
    "cnn_3.add(Dense(512,activation='sigmoid'))\n",
    "cnn_3.add(Dropout(0.2))\n",
    "cnn_3.add(Dense(10,activation='softmax'))\n",
    "print(cnn_3.summary())\n",
    "\n",
    "compile_model(cnn_3)\n",
    "h = fit_model(cnn_3,x_train,y_train,x_test,y_test)\n",
    "\n",
    "plt.plot(h.history['accuracy'])\n",
    "print('Test accuracy: %.2f %%'%(100*cnn_3.evaluate(x_test,y_test, verbose=0)[1]))\n",
    "\n",
    "plt.plot(h.history['accuracy'])\n",
    "plt.plot(h.history['val_accuracy'], 'r')\n",
    "plt.legend(['train acc', 'val acc'])\n",
    "print('train acc: %.2f %% \\nval acc: %.2f %%'%(h.history['accuracy'][-1]*100,h.history['val_accuracy'][-1]*100))\n",
    "\n",
    "plt.plot(np.log10(h.history['loss']))\n",
    "plt.plot(np.log10(h.history['val_loss']), 'r')\n",
    "plt.legend(['train loss', 'val loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
